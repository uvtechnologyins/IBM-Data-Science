{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uvtechnologyins/IBM-Data-Science/blob/main/adalflow_quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 🤗 Welcome to AdalFlow!\n",
        "## The PyTorch library to auto-optimize any LLM task pipelines\n",
        "\n",
        "Thanks for trying us out, we're here to provide you with the best LLM application development experience you can dream of 😊 any questions or concerns you may have, [come talk to us on discord,](https://discord.gg/ezzszrRZvT) we're always here to help! ⭐ <i>Star us on <a href=\"https://github.com/SylphAI-Inc/AdalFlow\">Github</a> </i> ⭐\n",
        "\n",
        "\n",
        "# Quick Links\n",
        "\n",
        "Github repo: https://github.com/SylphAI-Inc/AdalFlow\n",
        "\n",
        "Full Tutorials: https://adalflow.sylph.ai/index.html#.\n",
        "\n",
        "Deep dive on each API: check out the [developer notes](https://adalflow.sylph.ai/tutorials/index.html).\n",
        "\n",
        "Common use cases along with the auto-optimization:  check out [Use cases](https://adalflow.sylph.ai/use_cases/index.html).\n",
        "\n",
        "# Outline\n",
        "\n",
        "This is a quick introduction of what AdalFlow is capable of. We will cover:\n",
        "\n",
        "* Simple Chatbot with structured output\n",
        "* RAG task pipeline + Data processing pipeline\n",
        "* Agent\n",
        "\n",
        "**Next: Try our [auto-optimization](https://colab.research.google.com/drive/1n3mHUWekTEYHiBdYBTw43TKlPN41A9za?usp=sharing)**\n",
        "\n",
        "\n",
        "# Installation\n",
        "\n",
        "1. Use `pip` to install the `adalflow` Python package. We will need `openai`, `groq`, and `faiss`(cpu version) from the extra packages.\n",
        "\n",
        "  ```bash\n",
        "  pip install adalflow[openai,groq,faiss-cpu]\n",
        "  ```\n",
        "2. Setup  `openai` and `groq` API key in the environment variables"
      ],
      "metadata": {
        "id": "VVSOpjzJl_cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install adalflow[openai,groq,faiss-cpu]"
      ],
      "metadata": {
        "id": "JKK2ubhG08p3",
        "outputId": "e361b3a1-53c8-400f-e6bd-e00ef387fd50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adalflow[faiss-cpu,groq,openai]\n",
            "  Downloading adalflow-1.0.3-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from adalflow[faiss-cpu,groq,openai]) (6.0.2)\n",
            "Collecting backoff<3.0.0,>=2.2.1 (from adalflow[faiss-cpu,groq,openai])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting colorama<0.5.0,>=0.4.6 (from adalflow[faiss-cpu,groq,openai])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting diskcache<6.0.0,>=5.6.3 (from adalflow[faiss-cpu,groq,openai])\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting faiss-cpu>=1.8.0 (from adalflow[faiss-cpu,groq,openai])\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting groq>=0.9.0 (from adalflow[faiss-cpu,groq,openai])\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from adalflow[faiss-cpu,groq,openai]) (3.1.5)\n",
            "Collecting jsonlines<5.0.0,>=4.0.0 (from adalflow[faiss-cpu,groq,openai])\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from adalflow[faiss-cpu,groq,openai]) (1.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from adalflow[faiss-cpu,groq,openai]) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from adalflow[faiss-cpu,groq,openai]) (1.59.9)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from adalflow[faiss-cpu,groq,openai])\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting tiktoken>=0.3.3 (from adalflow[faiss-cpu,groq,openai])\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /usr/local/lib/python3.11/dist-packages (from adalflow[faiss-cpu,groq,openai]) (4.67.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.8.0->adalflow[faiss-cpu,groq,openai]) (24.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq>=0.9.0->adalflow[faiss-cpu,groq,openai]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq>=0.9.0->adalflow[faiss-cpu,groq,openai]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq>=0.9.0->adalflow[faiss-cpu,groq,openai]) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq>=0.9.0->adalflow[faiss-cpu,groq,openai]) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq>=0.9.0->adalflow[faiss-cpu,groq,openai]) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq>=0.9.0->adalflow[faiss-cpu,groq,openai]) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.3->adalflow[faiss-cpu,groq,openai]) (3.0.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines<5.0.0,>=4.0.0->adalflow[faiss-cpu,groq,openai]) (25.1.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.12.0->adalflow[faiss-cpu,groq,openai]) (0.8.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.3.3->adalflow[faiss-cpu,groq,openai]) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.3.3->adalflow[faiss-cpu,groq,openai]) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq>=0.9.0->adalflow[faiss-cpu,groq,openai]) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq>=0.9.0->adalflow[faiss-cpu,groq,openai]) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq>=0.9.0->adalflow[faiss-cpu,groq,openai]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq>=0.9.0->adalflow[faiss-cpu,groq,openai]) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq>=0.9.0->adalflow[faiss-cpu,groq,openai]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq>=0.9.0->adalflow[faiss-cpu,groq,openai]) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.3.3->adalflow[faiss-cpu,groq,openai]) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.3.3->adalflow[faiss-cpu,groq,openai]) (2.3.0)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading adalflow-1.0.3-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, jsonlines, faiss-cpu, diskcache, colorama, backoff, tiktoken, groq, adalflow\n",
            "Successfully installed adalflow-1.0.3 backoff-2.2.1 colorama-0.4.6 diskcache-5.6.3 faiss-cpu-1.10.0 groq-0.18.0 jsonlines-4.0.0 python-dotenv-1.0.1 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THTvmhjgfiHE"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install -U adalflow[openai,groq,faiss-cpu]\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# patch for colab to run\n",
        "\n",
        "!pip uninstall httpx anyio -y\n",
        "!pip install \"anyio>=3.1.0,<4.0\"\n",
        "!pip install httpx==0.24.1\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "RQZJtZmgqPwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Environment Variables\n",
        "\n",
        "Run the following code and pass your api key.\n",
        "\n",
        "Note: for normal `.py` projects, follow our [official installation guide](https://lightrag.sylph.ai/get_started/installation.html).\n",
        "\n",
        "*Go to [OpenAI](https://platform.openai.com/docs/introduction) and [Groq](https://console.groq.com/docs/) to get API keys if you don't already have.*"
      ],
      "metadata": {
        "id": "KapUyHMM07pJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt user to enter their API keys securely\n",
        "openai_api_key = getpass(\"Please enter your OpenAI API key: \")\n",
        "groq_api_key = getpass(\"Please enter your GROQ API key: \")\n",
        "\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "print(\"API keys have been set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONfzF9Puzdd_",
        "outputId": "a5f6c28f-4535-47be-e0d6-10ed701433d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your OpenAI API key: ··········\n",
            "Please enter your GROQ API key: ··········\n",
            "API keys have been set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 😇 First ChatBot\n",
        "\n",
        "We will start with a single turn chatbot which will explain concepts with ``explanation`` and ``example``. To achieve this, we will build a simple pipeline to get the **structured output** as ``QAOutput``.\n",
        "\n",
        "\n",
        "##Well-designed Base Classes\n",
        "\n",
        "\n",
        "We will use this use case to demonstrate how to leverage our two and only powerful base classes: `Component` as building blocks for the pipeline and `DataClass` to ease the data interaction with LLMs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SfGS7iddtfpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data and template [jinja2 syntax]\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict\n",
        "\n",
        "import adalflow as adal\n",
        "from adalflow.components.model_client import GroqAPIClient\n",
        "\n",
        "@dataclass\n",
        "class QAOutput(adal.DataClass):\n",
        "    explanation: str = field(\n",
        "        metadata={\"desc\": \"A brief explanation of the concept in one sentence.\"}\n",
        "    )\n",
        "    example: str = field(metadata={\"desc\": \"An example of the concept in a sentence.\"})\n",
        "    __output_fields__ = [\"explanation\", \"example\"] # this automatically controls output fields in the same order you provided\n",
        "\n",
        "\n",
        "\n",
        "qa_template = r\"\"\"<SYS>\n",
        "You are a helpful assistant.\n",
        "<OUTPUT_FORMAT>\n",
        "{{output_format_str}}\n",
        "</OUTPUT_FORMAT>\n",
        "</SYS>\n",
        "User: {{input_str}}\"\"\""
      ],
      "metadata": {
        "id": "kFMtZJcstwtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the task pipeline\n",
        "\n",
        "class QA(adal.Component):\n",
        "    def __init__(self, model_client: adal.ModelClient, model_kwargs: Dict):\n",
        "        super().__init__()\n",
        "\n",
        "        parser = adal.DataClassParser(data_class=QAOutput, return_data_class=True)\n",
        "        self.generator = adal.Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=qa_template,\n",
        "            prompt_kwargs={\"output_format_str\": parser.get_output_format_str()},\n",
        "            output_processors=parser,\n",
        "        )\n",
        "\n",
        "    def call(self, query: str):\n",
        "        return self.generator.call({\"input_str\": query})\n",
        "\n",
        "    async def acall(self, query: str):\n",
        "        return await self.generator.acall({\"input_str\": query})"
      ],
      "metadata": {
        "id": "bWgc1jU0u_jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clear Pipeline Structure\n",
        "\n",
        "Simply by using `print(qa)`, you can see the pipeline structure, which helps users understand any LLM workflow quickly, especially when the pipeline is complicated.\n",
        "\n"
      ],
      "metadata": {
        "id": "9DlqBW1luBN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the QA class\n",
        "\n",
        "qa = QA(\n",
        "    model_client=GroqAPIClient(),\n",
        "    model_kwargs={\"model\": \"llama3-8b-8192\"},\n",
        ")\n",
        "\n",
        "print(qa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1_lHozr8jfs",
        "outputId": "b4701699-c2df-47db-8edf-7b5b9992f43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_format_str: Your output should be formatted as a standard JSON instance with the following schema:\n",
            "```\n",
            "{\n",
            "    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\n",
            "    \"example\": \"An example of the concept in a sentence. (str) (required)\"\n",
            "}\n",
            "```\n",
            "-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n",
            "-Use double quotes for the keys and string values.\n",
            "-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\n",
            "-Follow the JSON formatting conventions.\n",
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n",
            "QA(\n",
            "  (generator): Generator(\n",
            "    model_kwargs={'model': 'llama3-8b-8192'}, \n",
            "    (prompt): Prompt(\n",
            "      template: <SYS>\n",
            "      You are a helpful assistant.\n",
            "      <OUTPUT_FORMAT>\n",
            "      {{output_format_str}}\n",
            "      </OUTPUT_FORMAT>\n",
            "      </SYS>\n",
            "      User: {{input_str}}, prompt_kwargs: {'output_format_str': 'Your output should be formatted as a standard JSON instance with the following schema:\\n```\\n{\\n    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\\n    \"example\": \"An example of the concept in a sentence. (str) (required)\"\\n}\\n```\\n-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\\n-Use double quotes for the keys and string values.\\n-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\\n-Follow the JSON formatting conventions.'}, prompt_variables: ['output_format_str', 'input_str']\n",
            "    )\n",
            "    (model_client): GroqAPIClient()\n",
            "    (output_processors): DataClassParser(\n",
            "      data_class=QAOutput, format_type=json,            return_data_class=True, input_fields=[],            output_fields=['explanation', 'example']\n",
            "      (_output_processor): JsonParser()\n",
            "      (output_format_prompt): Prompt(\n",
            "        template: Your output should be formatted as a standard JSON instance with the following schema:\n",
            "        ```\n",
            "        {{schema}}\n",
            "        ```\n",
            "        -Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n",
            "        -Use double quotes for the keys and string values.\n",
            "        -DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\n",
            "        -Follow the JSON formatting conventions., prompt_variables: ['schema']\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call the qa and check the output\n",
        "\n",
        "qa(\"What is LLM?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1ly8QgjGBB6",
        "outputId": "8941b4cc-7885-444b-ccc8-3089bbe07168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorOutput(id=None, data=QAOutput(explanation='LLM stands for Large Language Model, a type of artificial intelligence designed to process and generate human-like language.', example='LLMs are often used in applications such as language translation, text generation, and chatbots.'), error=None, usage=CompletionUsage(completion_tokens=58, prompt_tokens=170, total_tokens=228), raw_response='```\\n{\\n    \"explanation\": \"LLM stands for Large Language Model, a type of artificial intelligence designed to process and generate human-like language.\",\\n    \"example\": \"LLMs are often used in applications such as language translation, text generation, and chatbots.\"\\n}\\n```', metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display the prompt only\n",
        "\n",
        "qa.generator.print_prompt(\n",
        "        output_format_str=qa.generator.output_processors.get_output_format_str(),\n",
        "        input_str=\"What is LLM?\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "o0Np93NtGGy2",
        "outputId": "95243439-740e-48b3-e1af-02ec6eceb65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_format_str: Your output should be formatted as a standard JSON instance with the following schema:\n",
            "```\n",
            "{\n",
            "    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\n",
            "    \"example\": \"An example of the concept in a sentence. (str) (required)\"\n",
            "}\n",
            "```\n",
            "-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n",
            "-Use double quotes for the keys and string values.\n",
            "-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\n",
            "-Follow the JSON formatting conventions.\n",
            "Prompt:\n",
            "______________________\n",
            "<SYS>\n",
            "You are a helpful assistant.\n",
            "<OUTPUT_FORMAT>\n",
            "Your output should be formatted as a standard JSON instance with the following schema:\n",
            "```\n",
            "{\n",
            "    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\n",
            "    \"example\": \"An example of the concept in a sentence. (str) (required)\"\n",
            "}\n",
            "```\n",
            "-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n",
            "-Use double quotes for the keys and string values.\n",
            "-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\n",
            "-Follow the JSON formatting conventions.\n",
            "</OUTPUT_FORMAT>\n",
            "</SYS>\n",
            "User: What is LLM?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<SYS>\\nYou are a helpful assistant.\\n<OUTPUT_FORMAT>\\nYour output should be formatted as a standard JSON instance with the following schema:\\n```\\n{\\n    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\\n    \"example\": \"An example of the concept in a sentence. (str) (required)\"\\n}\\n```\\n-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\\n-Use double quotes for the keys and string values.\\n-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\\n-Follow the JSON formatting conventions.\\n</OUTPUT_FORMAT>\\n</SYS>\\nUser: What is LLM?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-Agnostic\n",
        "\n",
        "You can switch to any model simply by using a different model_client (provider) and model_kwargs.\n",
        "Let's use OpenAI's gpt-3.5-turbo model on the same pipeline."
      ],
      "metadata": {
        "id": "6OYUq0iBVHXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from adalflow.components.model_client import OpenAIClient\n",
        "\n",
        "qa_with_gpt = QA(\n",
        "    model_client=OpenAIClient(),\n",
        "    model_kwargs={\"model\": \"gpt-3.5-turbo\"}\n",
        ")\n",
        "\n",
        "qa_with_gpt(\"What is LLM?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fqlYq8sGYKw",
        "outputId": "186241d2-3b29-4855-8fb3-f4047da83d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_format_str: Your output should be formatted as a standard JSON instance with the following schema:\n",
            "```\n",
            "{\n",
            "    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\n",
            "    \"example\": \"An example of the concept in a sentence. (str) (required)\"\n",
            "}\n",
            "```\n",
            "-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n",
            "-Use double quotes for the keys and string values.\n",
            "-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\n",
            "-Follow the JSON formatting conventions.\n",
            "cache_path: /root/.adalflow/cache_OpenAIClient_gpt-3.5-turbo.db\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorOutput(id=None, data=QAOutput(explanation='LLM stands for Large Language Model, which is a type of artificial intelligence model that can process and generate human language.', example='GPT-3 is an example of an LLM that is capable of generating human-like text in a variety of contexts.'), error=None, usage=CompletionUsage(completion_tokens=66, prompt_tokens=167, total_tokens=233), raw_response='```json\\n{\\n    \"explanation\": \"LLM stands for Large Language Model, which is a type of artificial intelligence model that can process and generate human language.\",\\n    \"example\": \"GPT-3 is an example of an LLM that is capable of generating human-like text in a variety of contexts.\"\\n}\\n```', metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤗 First RAG\n",
        "\n",
        "Different from other libraries, the RAG pipeline consists of (1) a task pipeline consists of a retriever and a generator (2) a data pipeline that works with local/cloud db to preprocess and persist data.\n",
        "\n",
        "This suits the real product environment. And if the data is embedded and used only once-off on the fly, you can always skip the data storage."
      ],
      "metadata": {
        "id": "aVOZV19irzpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Config\n",
        "\n",
        "We will put all configurations together in `config` as dict."
      ],
      "metadata": {
        "id": "WrUmNr8dVh1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configs = {\n",
        "    \"embedder\": {\n",
        "        \"batch_size\": 100,\n",
        "        \"model_kwargs\": {\n",
        "            \"model\": \"text-embedding-3-small\",\n",
        "            \"dimensions\": 256,\n",
        "            \"encoding_format\": \"float\",\n",
        "        },\n",
        "    },\n",
        "    \"retriever\": {\n",
        "        \"top_k\": 2,\n",
        "    },\n",
        "    \"generator\": {\n",
        "        \"model\": \"gpt-3.5-turbo\",\n",
        "        \"temperature\": 0.3,\n",
        "        \"stream\": False,\n",
        "    },\n",
        "    \"text_splitter\": {\n",
        "        \"split_by\": \"word\",\n",
        "        \"chunk_size\": 400,\n",
        "        \"chunk_overlap\": 200,\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "G2Kpc4d6Vq2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data pipeline\n",
        "\n",
        "We will use local data base `LocalDB` and `core.data_process` to create a data processing pipeline. This data pipeline will split documents into chunks and work with `LocalDB` to persis the transformed/processed documents in local file `index.faiss` (pickle format).\n",
        "\n",
        "Data pipeline requires a sequence of `Document` as inputs."
      ],
      "metadata": {
        "id": "vu1WsDy0Vtdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from adalflow.components.data_process import (\n",
        "    RetrieverOutputToContextStr,\n",
        "    ToEmbeddings,\n",
        "    TextSplitter,\n",
        ")\n",
        "\n",
        "from adalflow.core.types import Document, ModelClientType\n",
        "\n",
        "\n",
        "def prepare_data_pipeline():\n",
        "    splitter = TextSplitter(**configs[\"text_splitter\"])\n",
        "    embedder = adal.Embedder(\n",
        "        model_client=ModelClientType.OPENAI(),\n",
        "        model_kwargs=configs[\"embedder\"][\"model_kwargs\"],\n",
        "    )\n",
        "    embedder_transformer = ToEmbeddings(\n",
        "        embedder=embedder, batch_size=configs[\"embedder\"][\"batch_size\"]\n",
        "    )\n",
        "    data_transformer = adal.Sequential(splitter, embedder_transformer) # sequential will chain together splitter and embedder\n",
        "    return data_transformer"
      ],
      "metadata": {
        "id": "aOklnBPqVxb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transformer = prepare_data_pipeline()\n",
        "data_transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E24GznkJV1-q",
        "outputId": "5cd326b4-e8a9-4774-eb60-212d70028a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): TextSplitter(split_by=word, chunk_size=400, chunk_overlap=200)\n",
              "  (1): ToEmbeddings(\n",
              "    batch_size=100\n",
              "    (embedder): Embedder(\n",
              "      model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "      (model_client): OpenAIClient()\n",
              "    )\n",
              "    (batch_embedder): BatchEmbedder(\n",
              "      (embedder): Embedder(\n",
              "        model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "        (model_client): OpenAIClient()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare documents for data transformer\n",
        "\n",
        "doc1 = Document(\n",
        "        meta_data={\"title\": \"Li Yin's profile\"},\n",
        "        text=\"My name is Li Yin, I love rock climbing\" + \"lots of nonsense text\" * 500,\n",
        "        id=\"doc1\",\n",
        ")\n",
        "doc2 = Document(\n",
        "    meta_data={\"title\": \"Interviewing Li Yin\"},\n",
        "    text=\"lots of more nonsense text\" * 250\n",
        "    + \"Li Yin is an AI researcher and a software engineer\"\n",
        "    + \"lots of more nonsense text\" * 250,\n",
        "    id=\"doc2\",\n",
        ")\n",
        "\n",
        "doc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMr8U1zhWR8P",
        "outputId": "88f826cf-73ed-48fb-e636-01541ea7493b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id=doc1, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector=[], parent_doc_id=None, order=None, score=None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the data\n",
        "\n",
        "transformed_documents = data_transformer([doc1, doc2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBy8oIkLWtHQ",
        "outputId": "615feefb-8bb2-4785-d3bd-d98c842d67d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Splitting Documents in Batches: 100%|██████████| 1/1 [00:00<00:00, 110.29it/s]\n",
            "Batch embedding documents: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
            "Adding embeddings to documents from batch: 1it [00:00, 7002.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformed documents\n",
        "\n",
        "From the following visualization, we will see `doc1` is splitted into 7 chunks and `doc2` is splitted into 10 chunks. We get this relation from reading the `transformed_documents`, the `parent_doc_id` field.\n",
        "\n",
        "Note: For `text` and `vector`, we dont show the full text or the full vector as it is rather long. You can access each field directly to visualize the full value"
      ],
      "metadata": {
        "id": "I_7meYA9W_Oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the transformed data\n",
        "transformed_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lIqy3OqW5HJ",
        "outputId": "d54b1704-2a4d-4856-bde5-912255277253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id=38cf2f19-ceab-46f7-9702-0aa2fa80b06f, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None),\n",
              " Document(id=b0f13ef5-a6d9-42f7-865f-f3f9f9bfa2b1, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=1, score=None),\n",
              " Document(id=efe655dd-703b-4db6-bd99-1d13c6d43243, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=2, score=None),\n",
              " Document(id=b52d4e0e-686a-43f3-b5a2-1361231e17ec, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=3, score=None),\n",
              " Document(id=394531aa-2663-4157-8b42-9e9bd3c60bc2, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=4, score=None),\n",
              " Document(id=b8c52337-5946-4312-b5a1-ea0977054b61, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=5, score=None),\n",
              " Document(id=214c10ab-a65d-4e23-aee7-5b8929db90c0, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=6, score=None),\n",
              " Document(id=9bcbd5a9-2a71-42e7-a5e4-75c5097bde53, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=0, score=None),\n",
              " Document(id=ab0b194d-dce8-45d8-87b7-417b9d482b78, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=1, score=None),\n",
              " Document(id=96419ee9-618e-4a6b-8066-ea8a94e5f31c, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=2, score=None),\n",
              " Document(id=707e1e75-aaf2-45be-9417-5512252402bc, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=3, score=None),\n",
              " Document(id=fd4b7e87-a219-4a51-a62e-7f46c18700e9, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None),\n",
              " Document(id=a5cb7e6b-6578-4021-abae-3c21a68bdee9, text='textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsens...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=5, score=None),\n",
              " Document(id=db75d53b-67d0-4f13-9c9e-a570b8fdcef1, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=6, score=None),\n",
              " Document(id=57cac48d-af10-42ae-992f-af69a2bf3b25, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=7, score=None),\n",
              " Document(id=6546d978-959b-48d7-a317-af58307d2f4a, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=8, score=None),\n",
              " Document(id=a74338f5-5f78-4cf5-862a-22710f3f274c, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=9, score=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use LocalDB\n",
        "\n",
        "We will use localdb to manage the `documents`, `transformers`, and the persistance of the transformed documents. This resembles more of the production environment where the embeddings and documents are often handled in data base and can be reused to save cost."
      ],
      "metadata": {
        "id": "uKSZmRxeX5t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from adalflow.core.db import LocalDB\n",
        "\n",
        "\n",
        "def prepare_database_with_index(docs: List[Document], index_path: str = \"index.faiss\"):\n",
        "    if os.path.exists(index_path):\n",
        "        return None\n",
        "    db = LocalDB()\n",
        "    db.load(docs)\n",
        "    data_transformer = prepare_data_pipeline()\n",
        "    db.transform(data_transformer, key=\"data_transformer\")\n",
        "    # store\n",
        "    db.save_state(index_path)\n",
        "    print(db)"
      ],
      "metadata": {
        "id": "fhVKVN7MYVVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the database for retriever\n",
        "\n",
        "prepare_database_with_index([doc1, doc2], index_path=\"index.faiss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fviYAerVYjL_",
        "outputId": "bbccdf9e-ecdd-4c90-ddbb-ddaf9d18a7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Splitting Documents in Batches: 100%|██████████| 1/1 [00:00<00:00, 106.53it/s]\n",
            "Batch embedding documents: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n",
            "Adding embeddings to documents from batch: 1it [00:00, 8065.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LocalDB(name='LocalDB', items=[Document(id=doc1, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector=[], parent_doc_id=None, order=None, score=None), Document(id=doc2, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector=[], parent_doc_id=None, order=None, score=None)], transformed_items={'data_transformer': [Document(id=7a526a52-76f0-4bd5-bd41-d02facb282af, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None), Document(id=40ce0741-d9f2-4243-a26b-32e1fddd8f71, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=1, score=None), Document(id=bb2fdb99-055e-456f-9798-a45dc521cafc, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=2, score=None), Document(id=6d7127fd-ad20-4e7c-9ce1-e188dd44776a, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=3, score=None), Document(id=77254de4-f896-4d07-94f6-f50c97a2f814, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=4, score=None), Document(id=1875f9ba-4bdb-443d-ae5a-6e9a688cf97a, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=5, score=None), Document(id=0750574e-fe16-4225-8f48-51d62da5fbc8, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=6, score=None), Document(id=ed9f429d-9bb6-4079-90fc-b9b360e922c5, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=0, score=None), Document(id=6d50c954-5ee0-4fc4-a05b-bf5eafa88f2f, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=1, score=None), Document(id=992405d4-41a4-49c5-bc67-e5e91059ee9f, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=2, score=None), Document(id=076a0ec5-f7ec-4cf0-990f-393a5db24fda, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=3, score=None), Document(id=b05e9391-0943-46e2-b107-e25150325fbe, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None), Document(id=10026653-2a01-45f3-b324-ce4f431863ff, text='textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsens...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=5, score=None), Document(id=d1a03698-d47b-41f2-bb19-9eaed00a3321, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=6, score=None), Document(id=12f776fe-bbe5-4af3-973d-584c886e7667, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=7, score=None), Document(id=1b1dece8-34ba-47e5-b72c-89a77b7520e4, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=8, score=None), Document(id=8cb220ee-bd36-40c8-92c1-449722efd300, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=9, score=None)]}, transformer_setups={'data_transformer': Sequential(\n",
            "  (0): TextSplitter(split_by=word, chunk_size=400, chunk_overlap=200)\n",
            "  (1): ToEmbeddings(\n",
            "    batch_size=100\n",
            "    (embedder): Embedder(\n",
            "      model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
            "      (model_client): OpenAIClient()\n",
            "    )\n",
            "    (batch_embedder): BatchEmbedder(\n",
            "      (embedder): Embedder(\n",
            "        model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
            "        (model_client): OpenAIClient()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")}, mapper_setups={})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load from file\n",
        "\n",
        "LocalDB `save_state` not only persist the transformed documents, but also the `data_transformer`.\n",
        "\n",
        "This is really helpful as your retriever needs to have a matching `embedder` to embed the string query. Saving the transformer lets you verify and know what embedder you need to pass to Retriever."
      ],
      "metadata": {
        "id": "a-97wL5gZZKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the database loading\n",
        "\n",
        "db = LocalDB.load_state(\"index.faiss\")\n",
        "db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmKZnA-YZNa1",
        "outputId": "ea046f18-6481-4739-a1a4-697f1c02495f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LocalDB(name='LocalDB', items=[Document(id=doc1, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector=[], parent_doc_id=None, order=None, score=None), Document(id=doc2, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector=[], parent_doc_id=None, order=None, score=None)], transformed_items={'data_transformer': [Document(id=7a526a52-76f0-4bd5-bd41-d02facb282af, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None), Document(id=40ce0741-d9f2-4243-a26b-32e1fddd8f71, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=1, score=None), Document(id=bb2fdb99-055e-456f-9798-a45dc521cafc, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=2, score=None), Document(id=6d7127fd-ad20-4e7c-9ce1-e188dd44776a, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=3, score=None), Document(id=77254de4-f896-4d07-94f6-f50c97a2f814, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=4, score=None), Document(id=1875f9ba-4bdb-443d-ae5a-6e9a688cf97a, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=5, score=None), Document(id=0750574e-fe16-4225-8f48-51d62da5fbc8, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=6, score=None), Document(id=ed9f429d-9bb6-4079-90fc-b9b360e922c5, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=0, score=None), Document(id=6d50c954-5ee0-4fc4-a05b-bf5eafa88f2f, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=1, score=None), Document(id=992405d4-41a4-49c5-bc67-e5e91059ee9f, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=2, score=None), Document(id=076a0ec5-f7ec-4cf0-990f-393a5db24fda, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=3, score=None), Document(id=b05e9391-0943-46e2-b107-e25150325fbe, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None), Document(id=10026653-2a01-45f3-b324-ce4f431863ff, text='textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsens...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=5, score=None), Document(id=d1a03698-d47b-41f2-bb19-9eaed00a3321, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=6, score=None), Document(id=12f776fe-bbe5-4af3-973d-584c886e7667, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=7, score=None), Document(id=1b1dece8-34ba-47e5-b72c-89a77b7520e4, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=8, score=None), Document(id=8cb220ee-bd36-40c8-92c1-449722efd300, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=9, score=None)]}, transformer_setups={'data_transformer': Sequential(\n",
              "  (0): TextSplitter(split_by=word, chunk_size=400, chunk_overlap=200)\n",
              "  (1): ToEmbeddings(\n",
              "    batch_size=100\n",
              "    (embedder): Embedder(\n",
              "      model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "      (model_client): OpenAIClient()\n",
              "    )\n",
              "    (batch_embedder): BatchEmbedder(\n",
              "      (embedder): Embedder(\n",
              "        model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "        (model_client): OpenAIClient()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")}, mapper_setups={})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test data fetching from local db\n",
        "\n",
        "db.get_transformed_data(\"data_transformer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymjXtUQ3axjw",
        "outputId": "51d5b996-eb62-4973-f9b3-02564a552ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id=4c1fa2e1-581d-4b57-bbb6-c8f5d5d4da4d, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None),\n",
              " Document(id=55ea86ea-175b-4a6b-ac4e-df303cb8440f, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=1, score=None),\n",
              " Document(id=464775cb-637a-43bc-be85-908a7a35e9c4, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=2, score=None),\n",
              " Document(id=0447254c-a093-4e56-9ad1-a7a8730742d0, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=3, score=None),\n",
              " Document(id=e61b5b84-6309-484b-84fc-8fcb1b1242fa, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=4, score=None),\n",
              " Document(id=8769e191-b63a-4467-a318-dc1bd74831c6, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=5, score=None),\n",
              " Document(id=1a0a5161-29b3-488c-aada-86fbf6f2df48, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=6, score=None),\n",
              " Document(id=7eb96fc2-116b-4c62-99f5-f123a18e2d4b, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=0, score=None),\n",
              " Document(id=16bb3433-b630-49f5-871d-6142fe67d8e6, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=1, score=None),\n",
              " Document(id=6c2c4ef5-c4e9-4a00-b890-c1d4626af208, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=2, score=None),\n",
              " Document(id=657ec50b-467d-4ab4-9d62-c9ee07e3316f, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=3, score=None),\n",
              " Document(id=090645e0-c346-428a-8419-ca21269746dc, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None),\n",
              " Document(id=f5f7f96c-083e-498f-ba96-136b05d4f26e, text='textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsens...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=5, score=None),\n",
              " Document(id=6ff3c50e-df07-4833-9008-ee4f0946b290, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=6, score=None),\n",
              " Document(id=4d4cfd5e-42dc-4598-822c-6e6f814ad37e, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=7, score=None),\n",
              " Document(id=e640cea8-a486-4229-8791-acf50665ceb7, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=8, score=None),\n",
              " Document(id=77513716-2dae-45db-9f60-7b5dd0f0d095, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=9, score=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG pipeline\n",
        "\n",
        "Now, we will create a RAG pipeline, it consists of:\n",
        "* db (we will load from index_path), we will use `data_transformer` as the key to load the transformed documents.\n",
        "* `FAISSRetriever` which will use embeddings to perform semantic search, and return similarity score in range [0, 1].\n",
        "* `RetrieverOutputToContextStr`: this will convert the retrieved documents to a single str.\n",
        "* `Generator`: we will use a simple `JsonParser` to output a dict with field `answer`."
      ],
      "metadata": {
        "id": "Ve655cL3Y1cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Any\n",
        "\n",
        "from adalflow.core.string_parser import JsonParser\n",
        "from adalflow.components.retriever.faiss_retriever import FAISSRetriever\n",
        "\n",
        "\n",
        "rag_prompt_task_desc = r\"\"\"\n",
        "You are a helpful assistant.\n",
        "\n",
        "Your task is to answer the query that may or may not come with context information.\n",
        "When context is provided, you should stick to the context and less on your prior knowledge to answer the query.\n",
        "\n",
        "Output JSON format:\n",
        "{\n",
        "    \"answer\": \"The answer to the query\",\n",
        "}\"\"\"\n",
        "\n",
        "\n",
        "class RAG(adal.Component):\n",
        "\n",
        "    def __init__(self, index_path: str = \"index.faiss\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.db = LocalDB.load_state(index_path)\n",
        "\n",
        "        self.transformed_docs: List[Document] = self.db.get_transformed_data(\n",
        "            \"data_transformer\"\n",
        "        )\n",
        "        embedder = adal.Embedder(\n",
        "            model_client=ModelClientType.OPENAI(),\n",
        "            model_kwargs=configs[\"embedder\"][\"model_kwargs\"],\n",
        "        )\n",
        "        # map the documents to embeddings\n",
        "        self.retriever = FAISSRetriever(\n",
        "            **configs[\"retriever\"],\n",
        "            embedder=embedder,\n",
        "            documents=self.transformed_docs,\n",
        "            document_map_func=lambda doc: doc.vector,\n",
        "        )\n",
        "        self.retriever_output_processors = RetrieverOutputToContextStr(deduplicate=True)\n",
        "\n",
        "        self.generator = adal.Generator(\n",
        "            prompt_kwargs={\n",
        "                \"task_desc_str\": rag_prompt_task_desc,\n",
        "            },\n",
        "            model_client=OpenAIClient(),\n",
        "            model_kwargs=configs[\"generator\"],\n",
        "            output_processors=JsonParser(),\n",
        "        )\n",
        "\n",
        "    def generate(self, query: str, context: Optional[str] = None) -> Any:\n",
        "        if not self.generator:\n",
        "            raise ValueError(\"Generator is not set\")\n",
        "\n",
        "        prompt_kwargs = {\n",
        "            \"context_str\": context,\n",
        "            \"input_str\": query,\n",
        "        }\n",
        "        response = self.generator(prompt_kwargs=prompt_kwargs)\n",
        "        return response\n",
        "\n",
        "    def call(self, query: str) -> Any:\n",
        "        retrieved_documents = self.retriever(query)\n",
        "        # fill in the document\n",
        "        for i, retriever_output in enumerate(retrieved_documents):\n",
        "            retrieved_documents[i].documents = [\n",
        "                self.transformed_docs[doc_index]\n",
        "                for doc_index in retriever_output.doc_indices\n",
        "            ]\n",
        "\n",
        "        print(f\"retrieved_documents: \\n {retrieved_documents}\\n\")\n",
        "        context_str = self.retriever_output_processors(retrieved_documents)\n",
        "\n",
        "        print(f\"context_str: \\n {context_str}\\n\")\n",
        "\n",
        "        return self.generate(query, context=context_str), retrieved_documents"
      ],
      "metadata": {
        "id": "LdoWsK6dbr0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize rag and visualize its structure\n",
        "\n",
        "rag = RAG(index_path=\"index.faiss\")\n",
        "rag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgFlKDDTcEEN",
        "outputId": "99c72b2b-b227-43c0-ff3f-574ef8643df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_OpenAIClient_gpt-3.5-turbo.db\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RAG(\n",
              "  (db): LocalDB(name='LocalDB', items=[Document(id=doc1, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector=[], parent_doc_id=None, order=None, score=None), Document(id=doc2, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector=[], parent_doc_id=None, order=None, score=None)], transformed_items={'data_transformer': [Document(id=7a526a52-76f0-4bd5-bd41-d02facb282af, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None), Document(id=40ce0741-d9f2-4243-a26b-32e1fddd8f71, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=1, score=None), Document(id=bb2fdb99-055e-456f-9798-a45dc521cafc, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=2, score=None), Document(id=6d7127fd-ad20-4e7c-9ce1-e188dd44776a, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=3, score=None), Document(id=77254de4-f896-4d07-94f6-f50c97a2f814, text='textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nons...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=4, score=None), Document(id=1875f9ba-4bdb-443d-ae5a-6e9a688cf97a, text='nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlot...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=5, score=None), Document(id=0750574e-fe16-4225-8f48-51d62da5fbc8, text='of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense text...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=6, score=None), Document(id=ed9f429d-9bb6-4079-90fc-b9b360e922c5, text='lots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense ...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=0, score=None), Document(id=6d50c954-5ee0-4fc4-a05b-bf5eafa88f2f, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=1, score=None), Document(id=992405d4-41a4-49c5-bc67-e5e91059ee9f, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=2, score=None), Document(id=076a0ec5-f7ec-4cf0-990f-393a5db24fda, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=3, score=None), Document(id=b05e9391-0943-46e2-b107-e25150325fbe, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None), Document(id=10026653-2a01-45f3-b324-ce4f431863ff, text='textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsens...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=5, score=None), Document(id=d1a03698-d47b-41f2-bb19-9eaed00a3321, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=6, score=None), Document(id=12f776fe-bbe5-4af3-973d-584c886e7667, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=7, score=None), Document(id=1b1dece8-34ba-47e5-b72c-89a77b7520e4, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=8, score=None), Document(id=8cb220ee-bd36-40c8-92c1-449722efd300, text='nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of m...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=9, score=None)]}, transformer_setups={'data_transformer': Sequential(\n",
              "    (0): TextSplitter(split_by=word, chunk_size=400, chunk_overlap=200)\n",
              "    (1): ToEmbeddings(\n",
              "      batch_size=100\n",
              "      (embedder): Embedder(\n",
              "        model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "        (model_client): OpenAIClient()\n",
              "      )\n",
              "      (batch_embedder): BatchEmbedder(\n",
              "        (embedder): Embedder(\n",
              "          model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "          (model_client): OpenAIClient()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )}, mapper_setups={})\n",
              "  (retriever): FAISSRetriever(\n",
              "    top_k=2, metric=prob, dimensions=256, total_documents=17\n",
              "    (embedder): Embedder(\n",
              "      model_kwargs={'model': 'text-embedding-3-small', 'dimensions': 256, 'encoding_format': 'float'}, \n",
              "      (model_client): OpenAIClient()\n",
              "    )\n",
              "  )\n",
              "  (retriever_output_processors): RetrieverOutputToContextStr(deduplicate=True)\n",
              "  (generator): Generator(\n",
              "    model_kwargs={'model': 'gpt-3.5-turbo', 'temperature': 0.3, 'stream': False}, \n",
              "    (prompt): Prompt(\n",
              "      template: <START_OF_SYSTEM_PROMPT>\n",
              "      {# task desc #}\n",
              "      {% if task_desc_str %}\n",
              "      {{task_desc_str}}\n",
              "      {% else %}\n",
              "      You are a helpful assistant.\n",
              "      {% endif %}\n",
              "      {#input format#}\n",
              "      {% if input_format_str %}\n",
              "      <INPUT_FORMAT>\n",
              "      {{input_format_str}}\n",
              "      </INPUT_FORMAT>\n",
              "      {% endif %}\n",
              "      {# output format #}\n",
              "      {% if output_format_str %}\n",
              "      <OUTPUT_FORMAT>\n",
              "      {{output_format_str}}\n",
              "      </OUTPUT_FORMAT>\n",
              "      {% endif %}\n",
              "      {# tools #}\n",
              "      {% if tools_str %}\n",
              "      <TOOLS>\n",
              "      {{tools_str}}\n",
              "      </TOOLS>\n",
              "      {% endif %}\n",
              "      {# example #}\n",
              "      {% if examples_str %}\n",
              "      <EXAMPLES>\n",
              "      {{examples_str}}\n",
              "      </EXAMPLES>\n",
              "      {% endif %}\n",
              "      {# chat history #}\n",
              "      {% if chat_history_str %}\n",
              "      <CHAT_HISTORY>\n",
              "      {{chat_history_str}}\n",
              "      </CHAT_HISTORY>\n",
              "      {% endif %}\n",
              "      {#contex#}\n",
              "      {% if context_str %}\n",
              "      <CONTEXT>\n",
              "      {{context_str}}\n",
              "      </CONTEXT>\n",
              "      {% endif %}\n",
              "      <END_OF_SYSTEM_PROMPT>\n",
              "      <START_OF_USER_PROMPT>\n",
              "      {% if input_str %}\n",
              "      {{input_str}}\n",
              "      {% endif %}\n",
              "      <END_OF_USER_PROMPT>\n",
              "      {# steps #}\n",
              "      {% if steps_str %}\n",
              "      <START_OF_ASSISTANT_STEPS>\n",
              "      {{steps_str}}\n",
              "      <END_OF_ASSISTANT_STEPS>\n",
              "      {% endif %}\n",
              "      , prompt_kwargs: {'task_desc_str': '\\nYou are a helpful assistant.\\n\\nYour task is to answer the query that may or may not come with context information.\\nWhen context is provided, you should stick to the context and less on your prior knowledge to answer the query.\\n\\nOutput JSON format:\\n{\\n    \"answer\": \"The answer to the query\",\\n}'}, prompt_variables: ['output_format_str', 'steps_str', 'task_desc_str', 'input_format_str', 'examples_str', 'context_str', 'tools_str', 'chat_history_str', 'input_str']\n",
              "    )\n",
              "    (model_client): OpenAIClient()\n",
              "    (output_processors): JsonParser()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run RAG end to end\n",
        "\n",
        "query = \"What is Li Yin's hobby and profession?\"\n",
        "\n",
        "response, retrieved_documents = rag.call(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Hr2w1Lcgq6",
        "outputId": "f2c0875c-c231-49c6-e541-77e2a4db646f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calling the call method\n",
            "retrieved_documents: \n",
            " [RetrieverOutput(doc_indices=[0, 11], doc_scores=[0.7120000123977661, 0.6650000214576721], query=\"What is Li Yin's hobby and profession?\", documents=[Document(id=7a526a52-76f0-4bd5-bd41-d02facb282af, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None), Document(id=b05e9391-0943-46e2-b107-e25150325fbe, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None)])]\n",
            "\n",
            "context_str: \n",
            "  My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of  textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more \n",
            "\n",
            "GeneratorOutput(id=None, data={'answer': \"Li Yin's hobby is rock climbing and profession is AI researcher and software engineer.\"}, error=None, usage=CompletionUsage(completion_tokens=23, prompt_tokens=1145, total_tokens=1168), raw_response='{\\n    \"answer\": \"Li Yin\\'s hobby is rock climbing and profession is AI researcher and software engineer.\"\\n}', metadata=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the documents from retriever\n",
        "\n",
        "print(retrieved_documents[0].documents)\n",
        "\n",
        "text1, text2= retrieved_documents[0].documents[0].text, retrieved_documents[0].documents[1].text\n",
        "\n",
        "print(\"rock climbing\" in text1, text1)\n",
        "print(\"software engineer\" in text2, text2)\n",
        "\n",
        "\n",
        "# try to manually search software engineer in the printout, you will find the key word that match the retrieval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYS4aUndklVU",
        "outputId": "597f80ee-9b23-47db-9e60-e42a17933593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(id=7a526a52-76f0-4bd5-bd41-d02facb282af, text='My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense te...', meta_data={'title': \"Li Yin's profile\"}, vector='len: 256', parent_doc_id=doc1, order=0, score=None), Document(id=b05e9391-0943-46e2-b107-e25150325fbe, text='textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonse...', meta_data={'title': 'Interviewing Li Yin'}, vector='len: 256', parent_doc_id=doc2, order=4, score=None)]\n",
            "True My name is Li Yin, I love rock climbinglots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of nonsense textlots of \n",
            "True textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textLi Yin is an AI researcher and a software engineerlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more nonsense textlots of more \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤚 First Agent\n",
        "\n",
        "We will try React agent which calls tools sequentially. Please refer to our tutorials [tools](https://adalflow.sylph.ai/tutorials/tool_helper.html) and [agent](https://adalflow.sylph.ai/tutorials/agent.html) for more details."
      ],
      "metadata": {
        "id": "NRa6ieBNCoK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from adalflow.components.agent import ReActAgent\n",
        "from adalflow.core import ModelClientType\n",
        "\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "   \"\"\"\n",
        "   Multiply two numbers.\n",
        "   \"\"\"\n",
        "   return a * b\n",
        "\n",
        "async def add(a: int, b: int) -> int:\n",
        "   \"\"\"\n",
        "   Add two numbers.\n",
        "   \"\"\"\n",
        "   return a + b\n",
        "\n",
        "def divide(a: float, b: float) -> float:\n",
        "   \"\"\"\n",
        "   Divide two numbers.\n",
        "   \"\"\"\n",
        "   return float(a) / b\n",
        "\n",
        "llama3_model_kwargs = {\n",
        "   \"model\": \"llama3-70b-8192\",  # llama3 70b works better than 8b here.\n",
        "   \"temperature\": 0.0,\n",
        "}\n",
        "gpt_model_kwargs = {\n",
        "   \"model\": \"gpt-3.5-turbo\",\n",
        "   \"temperature\": 0.0,\n",
        "}"
      ],
      "metadata": {
        "id": "mb_r5WQWCsMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will compare our React agent's response with the vanilla LLM."
      ],
      "metadata": {
        "id": "9kRSSTXWCvql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_react_agent(model_client: adal.ModelClient, model_kwargs: dict):\n",
        "   tools = [multiply, add, divide]\n",
        "   queries = [\n",
        "      \"What is the capital of France? and what is 465 times 321 then add 95297 and then divide by 13.2?\",\n",
        "      \"Give me 5 words rhyming with cool, and make a 4-sentence poem using them\",\n",
        "   ]\n",
        "\n",
        "   # vanilla LLM\n",
        "   generator = adal.Generator(\n",
        "      model_client=model_client,\n",
        "      model_kwargs=model_kwargs,\n",
        "   )\n",
        "\n",
        "   # agent\n",
        "   react = ReActAgent(\n",
        "      max_steps=6,\n",
        "      add_llm_as_fallback=True,\n",
        "      tools=tools,\n",
        "      model_client=model_client,\n",
        "      model_kwargs=model_kwargs,\n",
        "   )\n",
        "   # print(react)\n",
        "\n",
        "   for query in queries:\n",
        "      print(f\"Query: {query}\")\n",
        "      agent_response = react.call(query)\n",
        "      llm_response = generator.call(prompt_kwargs={\"input_str\": query})\n",
        "      print(f\"Agent response: {agent_response}\")\n",
        "      print(f\"LLM response: {llm_response}\")\n",
        "      print(\"\")"
      ],
      "metadata": {
        "id": "kEmk1mszCx85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_react_agent(ModelClientType.GROQ(), llama3_model_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHmH0XT-AOWV",
        "outputId": "ef3d149e-a9c4-4b5a-dff8-9af1daed1f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-70b-8192.db\n",
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-70b-8192.db\n",
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-70b-8192.db\n",
            "Query: What is the capital of France? and what is 465 times 321 then add 95297 and then divide by 13.2?\n",
            "\u001b[31m2024-08-23 19:27:35 - [react.py:284:call] - input_query: What is the capital of France? and what is 465 times 321 then add 95297 and then divide by 13.2?\u001b[0m\n",
            "\u001b[34m2024-08-23 19:27:36 - [react.py:264:_run_one_step] - Step 1: \n",
            "StepOutput(step=1, action=FunctionExpression(thought=\"Let's break down the query into subqueries and start with the first one.\", action='llm_tool(input=\"What is the capital of France?\")'), function=Function(thought=None, name='llm_tool', args=[], kwargs={'input': 'What is the capital of France?'}), observation='The capital of France is Paris.')\n",
            "_______\n",
            "\u001b[0m\n",
            "\u001b[34m2024-08-23 19:27:36 - [react.py:264:_run_one_step] - Step 2: \n",
            "StepOutput(step=2, action=FunctionExpression(thought=\"Now, let's move on to the second subquery.\", action='multiply(a=465, b=321)'), function=Function(thought=None, name='multiply', args=[], kwargs={'a': 465, 'b': 321}), observation=149265)\n",
            "_______\n",
            "\u001b[0m\n",
            "\u001b[34m2024-08-23 19:27:37 - [react.py:264:_run_one_step] - Step 3: \n",
            "StepOutput(step=3, action=FunctionExpression(thought=\"Now, let's add 95297 to the result.\", action='add(a=149265, b=95297)'), function=Function(thought=None, name='add', args=[], kwargs={'a': 149265, 'b': 95297}), observation=244562)\n",
            "_______\n",
            "\u001b[0m\n",
            "\u001b[34m2024-08-23 19:27:37 - [react.py:264:_run_one_step] - Step 4: \n",
            "StepOutput(step=4, action=FunctionExpression(thought=\"Now, let's divide the result by 13.2.\", action='divide(a=244562, b=13.2)'), function=Function(thought=None, name='divide', args=[], kwargs={'a': 244562, 'b': 13.2}), observation=18527.424242424244)\n",
            "_______\n",
            "\u001b[0m\n",
            "\u001b[34m2024-08-23 19:27:37 - [react.py:264:_run_one_step] - Step 5: \n",
            "StepOutput(step=5, action=FunctionExpression(thought=\"Now, let's combine the answers of both subqueries.\", action='finish(answer=\"The capital of France is Paris. The result of the mathematical operation is 18527.424242424244.\")'), function=Function(thought=None, name='finish', args=[], kwargs={'answer': 'The capital of France is Paris. The result of the mathematical operation is 18527.424242424244.'}), observation='The capital of France is Paris. The result of the mathematical operation is 18527.424242424244.')\n",
            "_______\n",
            "\u001b[0m\n",
            "\u001b[32m2024-08-23 19:27:37 - [react.py:298:call] - answer:\n",
            " The capital of France is Paris. The result of the mathematical operation is 18527.424242424244.\u001b[0m\n",
            "Agent response: The capital of France is Paris. The result of the mathematical operation is 18527.424242424244.\n",
            "LLM response: GeneratorOutput(id=None, data=\"I'd be happy to help you with that!\\n\\nThe capital of France is Paris.\\n\\nNow, let's tackle the math problem:\\n\\n1. 465 × 321 = 149,265\\n2. Add 95,297 to that result: 149,265 + 95,297 = 244,562\\n3. Divide the result by 13.2: 244,562 ÷ 13.2 = 18,516.06\\n\\nSo, the answer to the math problem is approximately 18,516.06.\", error=None, usage=CompletionUsage(completion_tokens=112, prompt_tokens=73, total_tokens=185), raw_response=\"I'd be happy to help you with that!\\n\\nThe capital of France is Paris.\\n\\nNow, let's tackle the math problem:\\n\\n1. 465 × 321 = 149,265\\n2. Add 95,297 to that result: 149,265 + 95,297 = 244,562\\n3. Divide the result by 13.2: 244,562 ÷ 13.2 = 18,516.06\\n\\nSo, the answer to the math problem is approximately 18,516.06.\", metadata=None)\n",
            "\n",
            "Query: Give me 5 words rhyming with cool, and make a 4-sentence poem using them\n",
            "\u001b[31m2024-08-23 19:27:38 - [react.py:284:call] - input_query: Give me 5 words rhyming with cool, and make a 4-sentence poem using them\u001b[0m\n",
            "\u001b[34m2024-08-23 19:27:38 - [react.py:264:_run_one_step] - Step 1: \n",
            "StepOutput(step=1, action=FunctionExpression(thought=\"I need to find 5 words that rhyme with 'cool' to create a poem.\", action='llm_tool(input=\"Give me 5 words rhyming with cool\")'), function=Function(thought=None, name='llm_tool', args=[], kwargs={'input': 'Give me 5 words rhyming with cool'}), observation='Here are 5 words that rhyme with \"cool\":\\n\\n1. Rule\\n2. Tool\\n3. Fool\\n4. Pool\\n5. School')\n",
            "_______\n",
            "\u001b[0m\n",
            "\u001b[34m2024-08-23 19:27:48 - [react.py:264:_run_one_step] - Step 2: \n",
            "StepOutput(step=2, action=FunctionExpression(thought='Now that I have the rhyming words, I need to create a 4-sentence poem using them.', action='llm_tool(input=\"Create a 4-sentence poem using the words \\'rule\\', \\'tool\\', \\'fool\\', \\'pool\\', and \\'school\\'\")'), function=Function(thought=None, name='llm_tool', args=[], kwargs={'input': \"Create a 4-sentence poem using the words 'rule', 'tool', 'fool', 'pool', and 'school'\"}), observation=\"Here is a 4-sentence poem using the words 'rule', 'tool', 'fool', 'pool', and 'school':\\n\\nIn the classroom, we learn to rule,\\nWith a pencil as our trusty tool.\\nBut if we're not careful, we can be a fool,\\nAnd end up in the principal's office pool, after school.\")\n",
            "_______\n",
            "\u001b[0m\n",
            "\u001b[34m2024-08-23 19:27:59 - [react.py:264:_run_one_step] - Step 3: \n",
            "StepOutput(step=3, action=FunctionExpression(thought='I have finished the task.', action='finish(answer=\"Here is a 4-sentence poem using the words \\'rule\\', \\'tool\\', \\'fool\\', \\'pool\\', and \\'school\\': In the classroom, we learn to rule, With a pencil as our trusty tool. But if we\\'re not careful, we can be a fool, And end up in the principal\\'s office pool, after school.\")'), function=Function(thought=None, name='finish', args=[], kwargs={'answer': \"Here is a 4-sentence poem using the words 'rule', 'tool', 'fool', 'pool', and 'school': In the classroom, we learn to rule, With a pencil as our trusty tool. But if we're not careful, we can be a fool, And end up in the principal's office pool, after school.\"}), observation=\"Here is a 4-sentence poem using the words 'rule', 'tool', 'fool', 'pool', and 'school': In the classroom, we learn to rule, With a pencil as our trusty tool. But if we're not careful, we can be a fool, And end up in the principal's office pool, after school.\")\n",
            "_______\n",
            "\u001b[0m\n",
            "\u001b[32m2024-08-23 19:27:59 - [react.py:298:call] - answer:\n",
            " Here is a 4-sentence poem using the words 'rule', 'tool', 'fool', 'pool', and 'school': In the classroom, we learn to rule, With a pencil as our trusty tool. But if we're not careful, we can be a fool, And end up in the principal's office pool, after school.\u001b[0m\n",
            "Agent response: Here is a 4-sentence poem using the words 'rule', 'tool', 'fool', 'pool', and 'school': In the classroom, we learn to rule, With a pencil as our trusty tool. But if we're not careful, we can be a fool, And end up in the principal's office pool, after school.\n",
            "LLM response: GeneratorOutput(id=None, data='Here are 5 words that rhyme with \"cool\":\\n\\n1. rule\\n2. tool\\n3. fool\\n4. pool\\n5. school\\n\\nAnd here\\'s a 4-sentence poem using these words:\\n\\nIn the summer heat, I\\'d rather be in school,\\nWhere the air is cool and the learning is the rule.\\nBut if I had to choose, I\\'d take a dip in the pool,\\nAnd hope that I don\\'t become a fool with a rusty old tool.', error=None, usage=CompletionUsage(completion_tokens=100, prompt_tokens=66, total_tokens=166), raw_response='Here are 5 words that rhyme with \"cool\":\\n\\n1. rule\\n2. tool\\n3. fool\\n4. pool\\n5. school\\n\\nAnd here\\'s a 4-sentence poem using these words:\\n\\nIn the summer heat, I\\'d rather be in school,\\nWhere the air is cool and the learning is the rule.\\nBut if I had to choose, I\\'d take a dip in the pool,\\nAnd hope that I don\\'t become a fool with a rusty old tool.', metadata=None)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReActAgent itself is a task pipeline that consists of Generator and output paraser and a for loop to do multiple steps."
      ],
      "metadata": {
        "id": "RxSxBT5dtZg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets visualize react structure\n",
        "\n",
        "tools = [multiply]\n",
        "react = ReActAgent(\n",
        "      max_steps=6,\n",
        "      add_llm_as_fallback=True,\n",
        "      tools=tools,\n",
        "      model_client=ModelClientType.GROQ(),\n",
        "      model_kwargs=llama3_model_kwargs,\n",
        "   )\n",
        "\n",
        "print(react)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3R8BMIStmzq",
        "outputId": "d6934d03-ff4b-4038-ebb8-777030c993be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-70b-8192.db\n",
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-70b-8192.db\n",
            "ReActAgent(\n",
            "  max_steps=6, add_llm_as_fallback=True, \n",
            "  (tool_manager): ToolManager(Tools: [FunctionTool(fn: <function multiply at 0x7b75c25541f0>, async: False, definition: FunctionDefinition(func_name='multiply', func_desc='multiply(a: int, b: int) -> int\\n\\n   Multiply two numbers.\\n   ', func_parameters={'type': 'object', 'properties': {'a': {'type': 'int'}, 'b': {'type': 'int'}}, 'required': ['a', 'b']})), FunctionTool(fn: <function ReActAgent._init_tools.<locals>.llm_tool at 0x7b75bc9ce830>, async: False, definition: FunctionDefinition(func_name='llm_tool', func_desc=\"llm_tool(input: str) -> str\\nI answer any input query with llm's world knowledge. Use me as a fallback tool or when the query is simple.\", func_parameters={'type': 'object', 'properties': {'input': {'type': 'str'}}, 'required': ['input']})), FunctionTool(fn: <function ReActAgent._init_tools.<locals>.finish at 0x7b75bc9ce7a0>, async: False, definition: FunctionDefinition(func_name='finish', func_desc='finish(answer: str) -> str\\nFinish the task with answer.', func_parameters={'type': 'object', 'properties': {'answer': {'type': 'str'}}, 'required': ['answer']}))], Additional Context: {})\n",
            "  (planner): Generator(\n",
            "    model_kwargs={'model': 'llama3-70b-8192', 'temperature': 0.0}, \n",
            "    (prompt): Prompt(\n",
            "      template: <SYS>\n",
            "      {# role/task description #}\n",
            "      You are a helpful assistant.\n",
            "      Answer the user's query using the tools provided below with minimal steps and maximum accuracy.\n",
            "      {# REACT instructions #}\n",
            "      Each step you will read the previous Thought, Action, and Observation(execution result of the action) and then provide the next Thought and Action.\n",
            "      {# Tools #}\n",
            "      {% if tools %}\n",
            "      <TOOLS>\n",
            "      You available tools are:\n",
            "      {% for tool in tools %}\n",
            "      {{ loop.index }}.\n",
            "      {{tool}}\n",
            "      ------------------------\n",
            "      {% endfor %}\n",
            "      </TOOLS>\n",
            "      {% endif %}\n",
            "      {# output format and examples for output format #}\n",
            "      <OUTPUT_FORMAT>\n",
            "      {{output_format_str}}\n",
            "      </OUTPUT_FORMAT>\n",
            "      <TASK_SPEC>\n",
            "      {# Task specification to teach the agent how to think using 'divide and conquer' strategy #}\n",
            "      - For simple queries: Directly call the ``finish`` action and provide the answer.\n",
            "      - For complex queries:\n",
            "          - Step 1: Read the user query and potentially divide it into subqueries. And get started with the first subquery.\n",
            "          - Call one available tool at a time to solve each subquery/subquestion. \\\n",
            "          - At step 'finish', join all subqueries answers and finish the task.\n",
            "      Remember:\n",
            "      - Action must call one of the above tools with name. It can not be empty.\n",
            "      - You will always end with 'finish' action to finish the task. The answer can be the final answer or failure message.\n",
            "      </TASK_SPEC>\n",
            "      </SYS>\n",
            "      -----------------\n",
            "      User query:\n",
            "      {{ input_str }}\n",
            "      {# Step History #}\n",
            "      {% if step_history %}\n",
            "      <STEPS>\n",
            "      Your previous steps:\n",
            "      {% for history in step_history %}\n",
            "      Step {{ loop.index }}.\n",
            "      \"Thought\": \"{{history.action.thought}}\",\n",
            "      \"Action\": \"{{history.action.action}}\",\n",
            "      \"Observation\": \"{{history.observation}}\"\n",
            "      ------------------------\n",
            "      {% endfor %}\n",
            "      </STEPS>\n",
            "      {% endif %}\n",
            "      You:, prompt_kwargs: {'tools': ['func_name: multiply\\nfunc_desc: \"multiply(a: int, b: int) -> int\\\\n\\\\n   Multiply two numbers.\\\\n   \"\\nfunc_parameters:\\n  type: object\\n  properties:\\n    a:\\n      type: int\\n    b:\\n      type: int\\n  required:\\n  - a\\n  - b', \"func_name: llm_tool\\nfunc_desc: 'llm_tool(input: str) -> str\\n\\n  I answer any input query with llm''s world knowledge. Use me as a fallback tool\\n  or when the query is simple.'\\nfunc_parameters:\\n  type: object\\n  properties:\\n    input:\\n      type: str\\n  required:\\n  - input\", \"func_name: finish\\nfunc_desc: 'finish(answer: str) -> str\\n\\n  Finish the task with answer.'\\nfunc_parameters:\\n  type: object\\n  properties:\\n    answer:\\n      type: str\\n  required:\\n  - answer\"], 'output_format_str': 'Your output should be formatted as a standard JSON instance with the following schema:\\n```\\n{\\n    \"thought\": \"Why the function is called (Optional[str]) (optional)\",\\n    \"action\": \"FuncName(<kwargs>) Valid function call expression. Example: \\\\\"FuncName(a=1, b=2)\\\\\" Follow the data type specified in the function parameters.e.g. for Type object with x,y properties, use \\\\\"ObjectType(x=1, y=2) (str) (required)\"\\n}\\n```\\nExamples:\\n```\\n{\\n    \"thought\": \"I have finished the task.\",\\n    \"action\": \"finish(answer=\\\\\"final answer: \\'answer\\'\\\\\")\"\\n}\\n________\\n```\\n-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\\n-Use double quotes for the keys and string values.\\n-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\\n-Follow the JSON formatting conventions.'}, prompt_variables: ['tools', 'output_format_str', 'step_history', 'input_str']\n",
            "    )\n",
            "    (model_client): GroqAPIClient()\n",
            "    (output_processors): JsonOutputParser(\n",
            "      data_class=FunctionExpression, examples=[FunctionExpression(thought='I have finished the task.', action='finish(answer=\"final answer: \\'answer\\'\")')], exclude_fields=None,             include_fields=None, return_data_class=True\n",
            "      (output_format_prompt): Prompt(\n",
            "        template: Your output should be formatted as a standard JSON instance with the following schema:\n",
            "        ```\n",
            "        {{schema}}\n",
            "        ```\n",
            "        {% if example %}\n",
            "        Examples:\n",
            "        ```\n",
            "        {{example}}\n",
            "        ```\n",
            "        {% endif %}\n",
            "        -Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n",
            "        -Use double quotes for the keys and string values.\n",
            "        -DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\n",
            "        -Follow the JSON formatting conventions., prompt_variables: ['example', 'schema']\n",
            "      )\n",
            "      (output_processors): JsonParser()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next: Try our [auto-optimization](https://colab.research.google.com/drive/1n3mHUWekTEYHiBdYBTw43TKlPN41A9za?usp=sharing)"
      ],
      "metadata": {
        "id": "anfvoEkIOHod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Issues and feedback\n",
        "\n",
        "If you encounter any issues, please report them here: [GitHub Issues](https://github.com/SylphAI-Inc/LightRAG/issues).\n",
        "\n",
        "For feedback, you can use either the [GitHub discussions](https://github.com/SylphAI-Inc/LightRAG/discussions) or [Discord](https://discord.gg/ezzszrRZvT)."
      ],
      "metadata": {
        "id": "3Wnvqs3RyI_z"
      }
    }
  ]
}